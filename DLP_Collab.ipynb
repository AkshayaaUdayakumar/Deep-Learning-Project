{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10x5tnpwsXOZ"
      },
      "outputs": [],
      "source": [
        "#!pip3 install tensorflow-addons\n",
        "#!pip3 install json\n",
        "#!pip3 install keras\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/ProjectDL\n",
        "%cd training_set_task3\n",
        "path_train = '/content/drive/MyDrive/ProjectDL/training_set_task3/'\n",
        "path_test = '/content/drive/MyDrive/ProjectDL/test_set_task3/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY_MKbSOsjIP"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import cv2\n",
        "import string\n",
        "import numpy as np\n",
        "import itertools\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIUdCMjvWL1G"
      },
      "outputs": [],
      "source": [
        "# Load training data from txt file\n",
        "with open('/content/drive/MyDrive/ProjectDL/training_set_task3/training_set_task3.txt') as f:\n",
        "    train_data = json.load(f)\n",
        "\n",
        "# Initialize empty lists to store image, text, and label data\n",
        "images,texts,labels = [],[],[]\n",
        "\n",
        "# Iterate through training data and process image, text, and label data\n",
        "for data in train_data:\n",
        "    img = cv2.imread(path_train + data['image'])\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    images.append(img)\n",
        "    text = data['text'].replace('\\n', ' ')\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join(text.split()).lower()\n",
        "    texts.append(text)\n",
        "    labels.append(data['labels'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode_labels(labels):\n",
        "    all_labels = list(set(list(itertools.chain(*labels)))) + ['']\n",
        "    one_hot_labels = np.zeros((len(labels), len(all_labels)))\n",
        "    for i, label_list in enumerate(labels):\n",
        "        if label_list:\n",
        "            for label in label_list:\n",
        "                one_hot_labels[i, all_labels.index(label)] = 1\n",
        "        else:\n",
        "            one_hot_labels[i, -1] = 1\n",
        "    return one_hot_labels"
      ],
      "metadata": {
        "id": "DkMuUKCtVnQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = list(set(list(itertools.chain(*labels)))) + ['']\n",
        "one_hot_labels = one_hot_encode_labels(labels)\n",
        "one_hot_labels = np.array(one_hot_labels)\n",
        "print(one_hot_labels.shape)\n",
        "print(one_hot_labels)"
      ],
      "metadata": {
        "id": "875NLX7YtQ6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save data to json file\n",
        "t_data = {\n",
        "    'images': images.tolist(),\n",
        "    'texts': texts,\n",
        "    'labels_onehot': one_hot_labels.tolist(),\n",
        "    'labels_string': labels,\n",
        "    'all_labels': all_labels\n",
        "}\n",
        "\n",
        "with open('/content/drive/MyDrive/ProjectDL/training_set_task3/training_task3.json', 'w') as f:\n",
        "    json.dump(t_data, f)\n",
        "\n",
        "# Load data from json file\n",
        "with open('/content/drive/MyDrive/ProjectDL/training_set_task3/training_task3.json', 'r') as f:\n",
        "    t_data = json.load(f)\n",
        "\n",
        "\n",
        "train_images = np.array(t_data['images'])\n",
        "train_texts = t_data['texts']\n",
        "train_one_hot_labels = np.array(t_data['labels_onehot'])\n",
        "train_labels = t_data['labels_string'] \n",
        "train_all_labels = t_data['all_labels'] \n",
        "\n",
        "\n",
        "print(train_images.shape)\n",
        "print(train_images[1], train_labels[1])"
      ],
      "metadata": {
        "id": "BP3cZ5qeqPMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64cnqx-nt6ZB"
      },
      "outputs": [],
      "source": [
        "# Load testing data from txt file\n",
        "with open('/content/drive/MyDrive/ProjectDL/test_set_task3/test_set_task3.txt') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Initialize empty lists to store image, text, and label data\n",
        "test_images,test_texts,test_labels = [],[],[]\n",
        "\n",
        "# Iterate through testing data and process image, text, and label data\n",
        "for tdata in test_data:\n",
        "\n",
        "    img = cv2.imread(path_test + tdata['image'])\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    test_images.append(img)\n",
        "    text = tdata['text'].replace('\\n', ' ')\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join(text.split()).lower()\n",
        "    test_texts.append(text)\n",
        "    test_labels.append(tdata['labels'])\n",
        "\n",
        "#print(test_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5FBIWwht6Vh"
      },
      "outputs": [],
      "source": [
        "# Convert image data to numpy array\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "# Create one-hot encoded labels\n",
        "test_all_labels = list(set(list(itertools.chain(*test_labels))))\n",
        "test_all_labels.append('')\n",
        "test_one_hot_labels = []\n",
        "test_one_hot_labels = one_hot_encode_labels(test_labels)\n",
        "test_one_hot_labels = np.array(test_one_hot_labels)\n",
        "print(test_one_hot_labels.shape)\n",
        "print(test_one_hot_labels)\n",
        "\n",
        "# Save data to json file\n",
        "test_data = {\n",
        "    'images': test_images.tolist(),\n",
        "    'texts': test_texts,\n",
        "    'labels_onehot': test_one_hot_labels.tolist(),\n",
        "    'labels_string': test_labels,\n",
        "    'all_labels': test_all_labels\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/ProjectDL/training_task3.json', 'w') as f:\n",
        "    json.dump(test_data, f)\n",
        "\n",
        "# Load data from json file\n",
        "with open('/content/drive/MyDrive/ProjectDL/training_task3.json', 'r') as f:\n",
        "    test_data = json.load(f)\n",
        "\n"
      ],
      "metadata": {
        "id": "uvLZAiEfxJJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = np.array(test_data['images'])\n",
        "test_texts = test_data['texts']\n",
        "test_one_hot_labels = np.array(test_data['labels_onehot'])\n"
      ],
      "metadata": {
        "id": "DDMcWpyRxuOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_data['labels_string']\n",
        "test_all_labels = test_data['all_labels']\n",
        "\n",
        "num_classes = len(all_labels)\n",
        "input_shape = images.shape[1:]"
      ],
      "metadata": {
        "id": "MuHfuSmK0hgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAqueKDmzlgO"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "image_size = 100\n",
        "patch_size = 5\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  \n",
        "transformer_layers = 5\n",
        "mlp_head_units = [2048, 1024] \n",
        "x_train=images\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "\n",
        "data_augmentation.layers[0].adapt(x_train)\n",
        "count=0\n",
        "\n",
        "# code taken from keras documentation for vision transformers\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    global count\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu, name='feats'+str(count))(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        count+=1\n",
        "    return x\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPcZwkhHBJSb"
      },
      "outputs": [],
      "source": [
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yXKvD1YA9Ox"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs) # data augmentation wasn't used\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "    j=0\n",
        "    for _ in range(transformer_layers):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.2\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.2)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "        j+=1\n",
        "\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.2)(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.4)\n",
        "    logits = layers.Dense(num_classes,activation='sigmoid')(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n",
        "\n",
        "x_train=images\n",
        "y_train=one_hot_labels\n",
        "x_test=test_images\n",
        "y_test=test_one_hot_labels\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPennyMXCW_B"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "    model.summary()\n",
        "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=[\"categorical_accuracy\"])\n",
        "    history = model.fit(x=x_train,y=y_train,batch_size=batch_size,epochs=5,validation_data=[x_test,y_test])\n",
        "    return history\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "\n",
        "plt.title('model acc')\n",
        "plt.ylabel('acc')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n-jREBhv1CgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e5s1hkUBaXF"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import Model\n",
        "# take features from the model\n",
        "feature_network = Model(vit_classifier.input, vit_classifier.get_layer('feats13').output)\n",
        "X_train = feature_network.predict(images)\n",
        "X_valid = feature_network.predict(test_images)\n",
        "y_train=one_hot_labels\n",
        "y_valid=test_one_hot_labels\n",
        "#print(X_valid.shape,y_valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSxEvBsVN5jz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# create an instance of RandomForestClassifier with 5 trees\n",
        "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions on the train and validation sets\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_valid_pred = model.predict(X_valid)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "valid_acc = accuracy_score(y_valid, y_valid_pred)\n",
        "\n",
        "print(\"Training accuracy: \", train_acc)\n",
        "print(\"Validation accuracy: \", valid_acc)\n",
        "print(\"F1 score: \", f1_score(y_valid, y_valid_pred, average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGRpeI0nmAVu"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(C = 4, penalty='l2', solver = 'liblinear', random_state=40)\n",
        "\n",
        "model = OneVsRestClassifier(clf)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred_proba = model.predict_proba(X_train)\n",
        "y_valid_pred_proba = model.predict_proba(X_valid)\n",
        "\n",
        "y_pred = (y_train_pred_proba > 0.3) \n",
        "y_train_new=(y_train > 0.3) \n",
        "\n",
        "\n",
        "y_pred1 = (y_valid_pred_proba > 0.3) \n",
        "y_valid_new=(y_valid > 0.3) \n",
        "\n",
        "print(\"Training accuracy: \", accuracy_score(y_train_new, y_pred))\n",
        "print(\"Validation Accuracy: \", accuracy_score(y_valid_new, y_pred1))\n",
        "print(\"F1 score: \", f1_score(y_valid_new,y_pred1,average='weighted'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}